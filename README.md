# sbp24-llm-attack-defense-tutorial
Materials and paper list for the SBP-BRiMS 2024 tutorial: "Defending Against Generative AI Threats in NLP"


# Paper List (is being updated, please check back soon!)



## LLM Threats

### Part 1: Attacks on LLMs


### Part 2: Misuse of LLMs

1. Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges [[paper link]](https://arxiv.org/pdf/2403.18249)
2. Can LLM-generated Misinformation be Detected? [[paper link]](https://arxiv.org/pdf/2309.13788)
3. 

## LLM Defenses and Safety

### Tools

### Model Editing and Parameter-efficent methods

1. DeTox: Toxic Subspace Projection for Model Editing [[paper link]](https://arxiv.org/pdf/2405.13967)
2. Editing Models with Task Arithmetic [[paper link]](https://arxiv.org/pdf/2212.04089)
3. Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks [[paper link]](https://arxiv.org/pdf/2407.02855)
4. Model Surgery: Modulating LLMâ€™s Behavior Via Simple Parameter Editing [[paper link]](https://arxiv.org/pdf/2407.08770)
5. Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing [[paper link]](https://arxiv.org/pdf/2405.18166v1)
6. 

### Decoding-time methods

1. RAIN: Your Language Models Can Align Themselves without Finetuning [[paper link]](https://arxiv.org/pdf/2309.07124)
2. 

