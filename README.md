# sbp24-llm-attack-defense-tutorial
Materials and paper list for the SBP-BRiMS 2024 tutorial: "Defending Against Generative AI Threats in NLP". 

Tutorial authors and organizers:

1. Amrita Bhattacharjee, Arizona State University
2. Raha Moraffah, Worcester Polytechnic Institute
3. Christopher Parisien, NVIDIA
4. Huan Liu, Arizona State University


# Paper List 
:exclamation::exclamation: (being updated, please star/watch this repo and check back again soon!) :exclamation::exclamation:



## LLM Threats

### Part 1: Attacks on LLMs

1. Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection [[paper link]](https://arxiv.org/pdf/2302.12173)
2. Universal and Transferable Adversarial Attacks on Aligned Language Models [[paper link]](https://arxiv.org/pdf/2307.15043)

### Part 2: Misuse of LLMs

1. Defending Against Social Engineering Attacks in the Age of LLMs [[paper link]](https://arxiv.org/pdf/2406.12263)
2. Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges [[paper link]](https://arxiv.org/pdf/2403.18249)
3. Can LLM-generated Misinformation be Detected? [[paper link]](https://arxiv.org/pdf/2309.13788)
4. 

## LLM Defenses and Safety

### Tools

1. garak: LLM vulnerability scanner
   
- [[github repo]](https://github.com/leondz/garak)
- [[website]](https://garak.ai/)
- [[paper]](https://arxiv.org/pdf/2406.11036v1)
- [[docs]](https://docs.garak.ai/garak)

2. NVIDIA NeMo Guardrails

- [[github repo]](https://github.com/NVIDIA/NeMo-Guardrails)
- [[paper]](https://arxiv.org/pdf/2310.10501)

   

### Model Editing and Parameter-efficent methods

1. DeTox: Toxic Subspace Projection for Model Editing [[paper link]](https://arxiv.org/pdf/2405.13967)
2. Editing Models with Task Arithmetic [[paper link]](https://arxiv.org/pdf/2212.04089)
3. Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks [[paper link]](https://arxiv.org/pdf/2407.02855)
4. Model Surgery: Modulating LLM’s Behavior Via Simple Parameter Editing [[paper link]](https://arxiv.org/pdf/2407.08770)
5. Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing [[paper link]](https://arxiv.org/pdf/2405.18166v1)
6. 

### Decoding-time methods

1. RAIN: Your Language Models Can Align Themselves without Finetuning [[paper link]](https://arxiv.org/pdf/2309.07124)
2. Parameter-Efficient Detoxification with Contrastive Decoding [[paper link]](https://arxiv.org/pdf/2401.06947)

